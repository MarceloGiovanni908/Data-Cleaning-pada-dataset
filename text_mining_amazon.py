# -*- coding: utf-8 -*-
"""Text Mining Amazon

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kHLWjZrT54Qg2nWpdM_HD7UTOxxt3iaA
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
import re

data_amazon = pd.read_csv('C:/Users/LEGION 5 PRO/OneDrive/Documents/Semester 4/Temu Kembali Informasi/dataset 14/sentiment labelled sentences/datasets.csv', names=['text','label','1','2','3','4'], delimiter=None)

data_amazon.head()

# Combine values in each column, excluding NaN
combined_data = data_amazon.apply(lambda x: ' '.join([str(val) for val in x.dropna()]), axis=1)

# Print the combined data
combined_data.head()

# Tokenisasi teks
tokenized_data = combined_data.apply(word_tokenize)
#print(tokenized_data)
tokenized_data.head()

# Menghapus stop words
stop_words = set(stopwords.words('english'))
filtered_data1 = tokenized_data.apply(lambda x: [word for word in x if word.lower() not in stop_words])
filtered_data1.head()

# Lemmatisasi teks
lemmatizer = WordNetLemmatizer()
preprocessed_data = filtered_data1.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
preprocessed_data.head()

# Menghapus tanda baca
punctuation = set(string.punctuation)
filtered_data2 = preprocessed_data.apply(lambda x: [word for word in x if word not in punctuation and len(word) > 1])
filtered_data2.head()

# Menghapus kata yang tidak perlu
unnecessary_words = set(['the', 'a', 'an'])
filtered_data3 = filtered_data2.apply(lambda x: [word for word in x if word.lower() not in unnecessary_words])
filtered_data3.head()

# Menghapus elemen list kosong
filtered_data4 = filtered_data3.apply(lambda x: list(filter(None, x)))
filtered_data4.head()

# Menggabungkan kata-kata menjadi kalimat
processed_sentences = filtered_data4.apply(lambda x: ' '.join(x))
processed_sentences.head()

# Simpan hasil preprocessing ke dalam file CSV baru
processed_sentences.to_csv('C:/Users/LEGION 5 PRO/OneDrive/Documents/Semester 4/Temu Kembali Informasi/dataset 14/sentiment labelled sentences/new_datasets.csv', index=False)